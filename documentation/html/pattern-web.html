<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
    <title>pattern-web</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <link type="text/css" rel="stylesheet" href="../clips.css" />
    <style>
        /* Small fixes because we omit the online layout.css. */
        h3 { line-height: 1.3em; }
        #page { margin-left: auto; margin-right: auto; }
        #header, #header-inner { height: 175px; }
        #header { border-bottom: 1px solid #C6D4DD;  }
        table { border-collapse: collapse; }
    </style>
    <link href="../js/shCore.css" rel="stylesheet" type="text/css" />
    <link href="../js/shThemeDefault.css" rel="stylesheet" type="text/css" />
    <script language="javascript" src="../js/shCore.js"></script>
    <script language="javascript" src="../js/shBrushXml.js"></script>
    <script language="javascript" src="../js/shBrushJScript.js"></script>
    <script language="javascript" src="../js/shBrushPython.js"></script>
</head>
<body class="node-type-page one-sidebar sidebar-right section-pages">
    <div id="page">
    <div id="page-inner">
    <div id="header"><div id="header-inner"></div></div>
    <div id="content">
    <div id="content-inner">
    <div class="node node-type-page"
        <div class="node-inner">
        <div class="breadcrumb">View online at: <a href="http://www.clips.ua.ac.be/pages/pattern-web" class="noexternal" target="_blank">http://www.clips.ua.ac.be/pages/pattern-web</a></div>
        <h1>pattern.web</h1>
        <!-- Parsed from the online documentation. -->
        <div id="node-1355" class="node node-type-page"><div class="node-inner">
<div class="content">
<h3>The pattern.web module bundles robust tools for online data mining: asynchronous requests, a uniform API for various web services (Google, Bing, Yahoo, Twitter, Wikipedia, Flickr, RSS, Atom), a HTML DOM parser, HTML tag stripping functions, web crawler, webmail, caching mechanisms, Unicode support.</h3>
<p>It can be used by itself or with other <a href="pattern.html">pattern</a> modules: web | <a href="pattern-db.html">db</a>&nbsp;| <a href="pattern-en.html">en</a> | <a href="pattern-search.html">search</a> | <a href="pattern-vector.html">vector</a> | <a href="pattern-graph.html">graph</a>.</p>
<p><img src="../g/pattern_schema.gif" alt="" width="620" height="180" /></p>
<hr />
<h2>Documentation</h2>
<ul>
<li><a href="#URL">URL downloads</a></li>
<li><a href="#asynchronous">Asynchronous requests</a></li>
<li><a href="#services">Search engine + web services</a>&nbsp;<span class="smallcaps link-maintenance">(google, bing, yahoo, twitter, wikipedia, flickr)</span></li>
<li><a href="#sort">Web sort</a></li>
<li><a href="#plaintext">HTML to plaintext</a></li>
<li><a href="#DOM">HTML DOM parser</a></li>
<li><a href="#pdf">PDF parser</a></li>
<li><a href="#spider">Spider</a></li>
<li><a href="#mail">E-mail</a></li>
<li><a href="#locale">Locale</a></li>
<li><a href="#cache">Cache</a></li>
</ul>
<p>&nbsp;</p>
<hr />
<h2><a name="URL"></a>URL downloads</h2>
<p>The <span class="inline_code">URL</span> object is based on Python's <span class="inline_code">urllib2.Request</span> and offers a robust way of working with online content. It has a <span class="inline_code">URL.download()</span> method that retrieves the content associated with a web address. The <span class="inline_code">method</span> parameter determines how <span class="inline_code">query</span> data is encoded:</p>
<ul>
<li><span class="inline_code">GET</span>: query is encoded in the URL string (typically used for retrieving data).</li>
<li><span class="inline_code">POST</span>: query is encoded in the message body (used for posting data).</li>
</ul>
<pre class="brush:python; gutter:false; light:true;">url = URL(string=&#39;&#39;, method=GET, query={})
</pre><pre class="brush:python; gutter:false; light:true;">url.string                  # u&#39;http://user:pw@domain.com:30/path/page?p=1#anchor&#39;
url.parts                   # Dictionary of attributes:</pre><pre class="brush:python; gutter:false; light:true;">url.protocol                # u&#39;http&#39;
url.username                # u&#39;user&#39;
url.password                # u&#39;pw&#39;
url.domain                   # u&#39;domain.com&#39;
url.port                     # 30
url.path                     # [u&#39;path&#39;]
url.page                     # u&#39;page&#39;
url.query                   # {u&#39;p&#39;: 1}
url.querystring             # u&#39;p=1&#39;
url.anchor                  # u&#39;anchor&#39;</pre><pre class="brush:python; gutter:false; light:true;">url.exists                  # False if URL.open() raises a HTTP404NotFound.
url.redirect                # Actual URL after redirection, or None.
url.headers                 # Dictionary of HTTP response headers.
url.mimetype                # Document MIME-type.</pre><pre class="brush:python; gutter:false; light:true;">url.open(timeout=10, proxy=None)
url.download(timeout=10, cached=True, throttle=0, proxy=None, unicode=False)
url.copy() </pre><ul>
<li><span class="inline_code">URL()</span> expects a string starting with a valid protocol (e.g. <span class="inline_code">http://</span>).<span class="inline_code">&nbsp;</span></li>
<li><span class="inline_code">URL.open()</span> returns a connection from which data can be retrieved with <span class="inline_code">connection.read()</span>.</li>
<li><span class="inline_code">URL.download()</span> will cache the retrieved data locally by default (faster next time). <br />Raises a <span class="inline_code">URLTimeoutError</span> if the download takes longer than the given <span class="inline_code">timeout</span>.<br />Sleeps for <span class="inline_code">throttle</span> seconds after the download is complete.<br />A proxy server can be given as a <span class="inline_code">(host, protocol)</span>-tuple, e.g., <span class="inline_code">("proxy.com", "https")</span>.<br />With <span class="inline_code">unicode=True</span>, returns the data as a Unicode string. By default it is <span class="inline_code">False</span> (data can be an image, for example) but&nbsp;<span class="inline_code">unicode=True</span> is advised for HTML.</li>
</ul>
<p>The following example downloads an image. <br />The helper function <span class="inline_code">extension()</span> parses the file extension from a file name:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.web import URL, extension
&gt;&gt;&gt; url = URL(&#39;http://www.clips.ua.ac.be/media/pattern_schema.gif&#39;)
&gt;&gt;&gt; f = open(&#39;test&#39; + extension(url.page), &#39;w&#39;) # save as test.gif
&gt;&gt;&gt; f.write(url.download())
&gt;&gt;&gt; f.close()</pre></div>
<h3>URL exceptions</h3>
<p><span class="inline_code">URL.open()</span> and <span class="inline_code">URL.download()</span> raise a <span class="inline_code">URLError</span> if something goes wrong. This is quite common since a lot of things can fail – no internet connection, server is down, etc.&nbsp; <span class="inline_code">URLError</span> has a number of subclasses that can help you figure out the error:</p>
<table class="border">
<tbody>
<tr>
<td><span class="smallcaps">Exception</span></td>
<td><span class="smallcaps">Description</span></td>
</tr>
<tr>
<td><span class="inline_code">URLError</span></td>
<td>URL contains errors (e.g. a missing <span class="inline_code">t</span> in <span class="inline_code">htp://</span>)</td>
</tr>
<tr>
<td><span class="inline_code">URLTimeout</span></td>
<td>URL takes too long to load.</td>
</tr>
<tr>
<td><span class="inline_code">HTTPError</span></td>
<td>URL causes an error on the contacted server.</td>
</tr>
<tr>
<td><span class="inline_code">HTTP400BadRequest</span></td>
<td>URL contains an invalid request.</td>
</tr>
<tr>
<td><span class="inline_code">HTTP401Authentication</span></td>
<td>URL requires a login and password.</td>
</tr>
<tr>
<td><span class="inline_code">HTTP403Forbidden</span></td>
<td>URL is not accessible (check user-agent).</td>
</tr>
<tr>
<td><span class="inline_code">HTTP404NotFound</span></td>
<td>URL doesn't exist on the internet.</td>
</tr>
</tbody>
</table>
<h3>URL mime-type</h3>
<p><span class="inline_code">URL.mimetype</span> can be used to check the type of content at the given address, which is more reliable than simply looking at the filename extension (which may be missing).</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; url = URL(&#39;http://www.clips.ua.ac.be/media/pattern_schema.gif&#39;)
&gt;&gt;&gt; print url.mimetype in MIMETYPE_IMAGE

True</pre></div>
<table class="border">
<tbody>
<tr>
<td><span class="smallcaps">Global</span></td>
<td><span class="smallcaps">Value</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_WEBPAGE</span></td>
<td><span class="inline_code">['text/html']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_STYLESHEET</span></td>
<td><span class="inline_code">['text/css']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_PLAINTEXT</span></td>
<td><span class="inline_code">['text/plain']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_PDF</span></td>
<td><span class="inline_code">['application/pdf']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_NEWSFEED</span></td>
<td><span class="inline_code">['application/rss+xml', 'application/atom+xml']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_IMAGE</span></td>
<td><span class="inline_code">['image/gif', 'image/jpeg', 'image/png']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_AUDIO</span></td>
<td><span class="inline_code">['audio/mpeg', 'audio/mp4', 'audio/x-wav']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_VIDEO</span></td>
<td><span class="inline_code">['video/mpeg', 'video/mp4', 'video/quicktime']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_ARCHIVE</span></td>
<td><span class="inline_code">['application/x-tar', 'application/zip']</span></td>
</tr>
<tr>
<td><span class="inline_code">MIMETYPE_SCRIPT</span></td>
<td><span class="inline_code">['application/javascript']</span></td>
</tr>
</tbody>
</table>
<h3>User-agent and referrer</h3>
<p><span class="inline_code">URL.open()</span> and <span class="inline_code">URL.download()</span> have two optional parameters: <span class="inline_code">user_agent</span> and <span class="inline_code">referrer</span>, used to identify the application accessing the web. Some websites include code to block out any application except browsers. By setting a <span class="inline_code">user_agent</span> you can make the application appear as a browser. This is called <em>spoofing</em> and it is not encouraged, though sometimes necessary.</p>
<p>For example, to pose as a Firefox browser:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; URL(&#39;http://www.clips.ua.ac.be&#39;).download(user_agent=&#39;Mozilla/5.0&#39;)
</pre></div>
<h3>Find URLs</h3>
<p>The <span class="inline_code">find_urls()</span> function can be used to parse URLs from a text string. This will work on links starting with <span class="inline_code">http://</span>, <span class="inline_code">https://</span>, <span class="inline_code">www.</span> or domain names ending in <span class="inline_code">.com</span>, <span class="inline_code">.org</span>. <span class="inline_code">.net</span>. The parser will detect and strip leading punctuation (open parens) and trailing punctuation (period, comma, close parens). Comparably, the <span class="inline_code">find_email()</span> function can be used to parse e-mail addresses from a string.</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; print find_urls(&#39;Visit our website (www․clips.ua.ac.be)&#39;, unique=True)

[&#39;www.clips.ua.ac.be&#39;]
</pre></div>
<p>&nbsp;</p>
<hr />
<h2><a name="asynchronous"></a>Asynchronous requests</h2>
<p>The <span class="inline_code">asynchronous()</span> function can be used to execute a function in the background. It takes the function, its arguments and optional keyword arguments. It returns an <span class="inline_code">AsynchronousRequest</span> object that stores the given function's return value – once it is done. The main program can continue to run in the meantime.</p>
<pre class="brush:python; gutter:false; light:true;">request = asynchronous(function, *args, **kwargs)</pre><pre class="brush:python; gutter:false; light:true;">request.done                # True when the function is done.
request.elapsed             # Time running, in seconds.
request.value               # Function return value when done (or None).
request.error               # Function Exception (if any).
</pre><pre class="brush:python; gutter:false; light:true;">request.now()               # Waits for function and returns its value.
</pre><p>This is useful to execute a web query without hanging the user interface of your application, but instead display a progress bar (for example). The example below illustrates this. In a real-world setup you would poll <span class="inline_code">request.done</span> in the application's event loop:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; import time
&gt;&gt;&gt; request = asynchronous(Google().search, &#39;holy grail&#39;, timeout=4)
&gt;&gt;&gt; while not request.done:
&gt;&gt;&gt;    time.sleep(0.1)
&gt;&gt;&gt;    print &#39;busy...&#39;
&gt;&gt;&gt; print request.value
</pre></div>
<p>For a number of good reasons, there is no way to interrupt or "kill" a background process (i.e. a Python thread). You are responsible for ensuring that the given function doesn't hang.</p>
<p>&nbsp;</p>
<hr />
<h2><a name="services"></a>Search engine + web services</h2>
<p>The <span class="inline_code">SearchEngine</span> object offers a uniform way to address different web services, such as Google and Wikipedia. The <span class="inline_code">SearchEngine.search()</span> method returns a list of <span class="inline_code">Result</span> objects for a given query string – similar to a search field in a browser.</p>
<pre class="brush:python; gutter:false; light:true;">engine = SearchEngine(license=None, throttle=1.0, language=None)</pre><pre class="brush:python; gutter:false; light:true;">engine.license              # Service license key.
engine.throttle             # Time between requests (being nice to server).
engine.language             # Restriction for Result.language (e.g., &#39;en&#39;).</pre><pre class="brush:python; gutter:false; light:true;">engine.search(query, 
    type = SEARCH,          # SEARCH | IMAGE | NEWS
   start = 1,               # Starting page.
   count = 10,              # Results per page.
    sort = RELEVANCY,       # Results sort order: RELEVANCY | LATEST
    size = None             # Image size: TINY | SMALL | MEDIUM | LARGE
  cached = True)            # Cache locally?</pre><p><span class="small"><span style="text-decoration: underline;">Note</span>: <span class="inline_code">SearchEngine.search()</span> also takes the same optional parameters as <span class="inline_code">URL.download()</span>.</span></p>
<h3>Google | Yahoo | Bing | Twitter | Facebook | Wikipedia | Flickr</h3>
<p><span class="inline_code">SearchEngine</span> is subclassed by <span class="inline_code">Google</span>, <span class="inline_code">Yahoo</span>, <span class="inline_code">Bing</span>, <span class="inline_code">Twitter</span>, <span class="inline_code">Facebook</span>,&nbsp;<span class="inline_code">Wikipedia</span>, <span class="inline_code">Flickr</span>,&nbsp; <span class="inline_code">Newsfeed</span>:</p>
<pre class="brush:python; gutter:false; light:true;">engine = Google(license=None, throttle=0.5, language=None)</pre><pre class="brush:python; gutter:false; light:true;">engine = Yahoo(license=None, throttle=0.5, language=None)</pre><pre class="brush:python; gutter:false; light:true;">engine = Bing(license=None, throttle=0.5, language=None)</pre><pre class="brush:python; gutter:false; light:true;">engine = Twitter(license=None, throttle=0.5, language=None)</pre><pre class="brush:python; gutter:false; light:true;">engine = Facebook(license=None, throttle=1.0, language=&#39;en&#39;)</pre><pre class="brush:python; gutter:false; light:true;">engine = Wikipedia(license=None, throttle=5.0, language=None)</pre><pre class="brush:python; gutter:false; light:true;">engine = Flickr(license=None, throttle=5.0, language=None)</pre><pre class="brush:python; gutter:false; light:true;">engine = Newsfeed(license=None, throttle=1.0, language=None)</pre><p>Each of these has different settings for <span class="inline_code">search()</span>. For example,&nbsp;<span class="inline_code">Twitter.search()</span> returns up to 1500 results for a search term (15 queries with 100 results each, or 150 queries with 10 results each). It has an hourly limit of 150 queries (each call to <span class="inline_code">search()</span> counts as one query).</p>
<table class="border">
<tbody>
<tr>
<td><span class="smallcaps">Engine</span></td>
<td><span class="smallcaps">type</span></td>
<td><span class="smallcaps">start</span></td>
<td><span class="smallcaps">count</span></td>
<td><span class="smallcaps">sort</span></td>
<td><span class="smallcaps">limit</span></td>
<td><span class="smallcaps">throttle</span></td>
</tr>
<tr>
<td><span class="inline_code">Google</span></td>
<td><span class="inline_code">SEARCH<sup>1</sup></span></td>
<td>1-100/<span class="inline_code">count</span></td>
<td>1-10</td>
<td><span class="inline_code">RELEVANCY</span></td>
<td><span class="smallcaps">paid</span></td>
<td>0.5</td>
</tr>
<tr>
<td><span class="inline_code">Yahoo</span></td>
<td><span class="inline_code">SEARCH|IMAGE|NEWS<sup>12</sup></span></td>
<td>1-1000/count</td>
<td>1-50</td>
<td><span class="inline_code">RELEVANCY</span></td>
<td><span class="smallcaps">paid</span></td>
<td>0.5</td>
</tr>
<tr>
<td><span class="inline_code">Bing</span></td>
<td><span class="inline_code">SEARCH|IMAGE|NEWS<sup>3</sup></span></td>
<td>1-1000/<span class="inline_code">count</span></td>
<td>1-50</td>
<td><span class="inline_code">RELEVANCY</span></td>
<td class="smallcaps">paid</td>
<td>0.5</td>
</tr>
<tr>
<td><span class="inline_code">Twitter</span></td>
<td><span class="inline_code">SEARCH</span></td>
<td>1-1500/<span class="inline_code">count</span></td>
<td>1-100</td>
<td><span class="inline_code">RELEVANCY</span></td>
<td>150/hour</td>
<td>0.5</td>
</tr>
<tr>
<td><span class="inline_code">Facebook</span></td>
<td><span class="inline_code">SEARCH</span></td>
<td>1</td>
<td>1-100</td>
<td><span class="inline_code">RELEVANCT</span></td>
<td>500/hour</td>
<td>1.0</td>
</tr>
<tr>
<td><span style="font-family: Courier,monospace; font-size: small;"><span style="font-size: 12px;">Wikipedia</span></span></td>
<td><span style="font-family: Courier,monospace; font-size: small;"><span style="font-size: 12px;">SEARCH</span></span></td>
<td>1</td>
<td>1</td>
<td><span style="font-family: Courier,monospace; font-size: small;"><span style="font-size: 12px;">RELEVANCY</span></span></td>
<td>-</td>
<td>5.0</td>
</tr>
<tr>
<td><span style="font-family: Courier,monospace; font-size: small;"><span style="font-size: 12px;">Flickr<br /></span></span></td>
<td><span style="font-family: Courier,monospace; font-size: 12px;">IMAGE</span></td>
<td>1+</td>
<td>1-500</td>
<td><span style="font-family: Courier,monospace; font-size: 12px;">RELEVANCY|LATEST</span></td>
<td>-</td>
<td>5.0</td>
</tr>
<tr>
<td><span class="inline_code">Newsfeed</span></td>
<td><span class="inline_code">NEWS</span>&nbsp;</td>
<td>1+&nbsp;</td>
<td>-&nbsp;</td>
<td><span class="inline_code">LATEST</span>&nbsp;</td>
<td>?&nbsp;</td>
<td>1.0&nbsp;</td>
</tr>
</tbody>
</table>
<p><span class="small"><sup>1&nbsp;</sup><span class="inline_code">Google</span> and <span class="inline_code">Yahoo</span> are paid services – see further how to obtain a license key.<span style="color: #666666; font-size: x-small;"><span style="font-size: 10px;"><br /></span></span></span><span style="font-size: 11px;"><sup style="color: #666666;">2&nbsp;</sup><span style="font-family: Courier,monospace; font-size: 12px;">Yahoo.search(type=IMAGES)</span></span><span style="font-size: 11px;">&nbsp;has a&nbsp;<span class="inline_code" style="font-family: Courier, monospace; font-size: 12px;">count</span>&nbsp;of 1-35.<br /></span><span style="font-size: 11px;"><sup style="color: #666666;">3&nbsp;</sup><span style="font-family: Courier,monospace; font-size: 12px;">Bing.search(type=NEWS)</span></span><span style="font-size: 11px;">&nbsp;has a&nbsp;<span class="inline_code" style="font-family: Courier, monospace; font-size: 12px;">count</span>&nbsp;of 1-15.</span><span style="color: #666666; font-size: x-small;"><span style="font-size: 10px;"><br /></span></span></p>
<h3>Results</h3>
<p><span class="inline_code">SearchEngine.search()</span> returns a list of <span class="inline_code">Result</span> objects. The list has an additional <span class="inline_code">total</span> attribute, an estimate of the total number of results available for the given query. Each <span class="inline_code">Result</span> holds useful information:</p>
<pre class="brush:python; gutter:false; light:true;">result = Result(url)</pre><pre class="brush:python; gutter:false; light:true;">result.url                  # URL of content associated with the given query.
result.title                # Content title.
result.description          # Content summary.
result.language             # Content language.
result.author               # For news items and images.
result.date                 # For news items.</pre><pre class="brush:python; gutter:false; light:true;">result.download(timeout=10, cached=True, proxy=None)
</pre><ul>
<li>All attributes are Unicode strings.</li>
<li>The <span class="inline_code">Result.download()</span> method uses the <span class="inline_code">URL</span> object internally.<br />It takes the same optional parameters as <span class="inline_code">URL.download()</span>.</li>
</ul>
<p>For example:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; engine = Bing(license=None)
&gt;&gt;&gt; for i in range(1,5):
&gt;&gt;&gt;    for result in engine.search(&#39;holy handgrenade&#39;, type=SEARCH, start=i):
&gt;&gt;&gt;        print repr(plaintext(result.description))
&gt;&gt;&gt;        print

u&quot;The Holy Hand Grenade of Antioch is a fictional weapon from ...&quot;
u&#39;Once the number three, being the third number, be reached, then ...&#39;
</pre></div>
<p>&nbsp;</p>
<p><span class="smallcaps">Image search<br /></span></p>
<p>For <span class="inline_code">Yahoo</span>, <span class="inline_code">Bing</span> and <span class="inline_code">Flickr</span>, image links retrieved with <span class="inline_code">search(type=IMAGE)</span> can be filtered by setting the <span class="inline_code">size</span> parameter to <span style="font-family: Courier, monospace; font-size: 12px;">TINY<span style="font-family: Georgia, Times, serif; font-size: 13px;">&nbsp;|&nbsp;</span>SMALL<span style="font-family: Georgia, Times, serif; font-size: 13px;">&nbsp;|&nbsp;</span>MEDIUM<span style="font-family: Georgia, Times, serif; font-size: 13px;">&nbsp;|&nbsp;</span>LARGE</span> (or&nbsp;<span style="font-family: Courier, monospace; font-size: 12px;">None</span>&nbsp;for any size).&nbsp;Note that <span class="inline_code">Yahoo</span> image search has a <span class="inline_code">count</span> of 1-35.</p>
<p>For <span class="inline_code">Twitter</span>, each result has a <span class="inline_code">Result.profile</span> property with the URL to the user's profile picture.</p>
<p>The use of downloaded images may be restricted by authorship.&nbsp;<span class="inline_code">Flickr.search()</span> can be used with an optional <span class="inline_code">copyright=False</span>. This will only retrieve results with no copyright restrictions (either under Creative Commons license <a href="http://creativecommons.org/licenses/by-sa/2.0/" target="_blank">by-sa</a>&nbsp;or in the public domain).</p>
<p>&nbsp;</p>
<p><span class="smallcaps">Service license key</span></p>
<p>Some services require a license key. They may work without one, but this implies that you share a public license key (and query limit) with all other users of the web module. If you reach the query limit, <span class="inline_code">SearchEngine.search()</span> will raise an <span class="inline_code">SearchEngineLimitError</span>.</p>
<p><span class="inline_code">Google</span> is a paid service (5$ per 1o00 queries). If you have a license key you get 100 free queries per day. To get a key, follow the link below, activate "Custom Search API" and "Translate API" under "Services" and look up your key under "API Access".</p>
<p>Bing is a paid service (20$ per 10,000 queries). If you have a license key you get 5,000 free queries per month. To get a key, follow the link below.</p>
<p><span class="inline_code">Yahoo</span>&nbsp;is a paid service (0.8$ per 1000 queries) that requires an OAuth key + secret, which you can pass as a tuple to:&nbsp;<span class="inline_code">Yahoo(license=(key, secret))</span>.</p>
<p>Obtain a license key for: <a href="https://code.google.com/apis/console/" target="_blank">Google</a>,&nbsp;<a href="https://datamarket.azure.com/dataset/5BA839F1-12CE-4CCE-BF57-A49D98D29A44" target="_blank">Bing</a>, <a href="http://developer.yahoo.com/search/boss/" target="_blank">Yahoo</a>, <a href="http://www.flickr.com/services/api/keys/" target="_blank">Flickr</a>.</p>
<p>&nbsp;</p>
<p><span class="smallcaps">Service request throttle</span></p>
<p>A <span class="inline_code">SearchEngine.search()</span> request takes a minimum amount of time to complete, as outlined in the table above. This is intended as etiquette towards the servers providing the service. Be polite and raise the throttle value when you plan to run a lot of queries in batch.</p>
<p>Note that Wikipedia requests are especially intensive. If you plan to mine a lot of data from Wikipedia, download the <a href="http://en.wikipedia.org/wiki/Wikipedia:Database_download">Wikipedia database</a> instead.</p>
<p>&nbsp;</p>
<hr />
<h2>RSS + Atom newsfeeds</h2>
<p>The <span class="inline_code">Newsfeed</span> object is a wrapper around Mark Pilgrim's <a href="http://www.feedparser.org/" target="_blank">Universal Feed Parser</a>. <span class="inline_code">Newsfeed.search()</span> takes the web address of an RSS or Atom news feed and returns a list of <span class="inline_code">Result</span> objects. For example:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; NATURE = &#39;http://www.nature.com/nature/current_issue/rss/index.html&#39;
&gt;&gt;&gt; for result in Newsfeed().search(NATURE)[:5]:
&gt;&gt;&gt;     print repr(result.title)

u&#39;Biopiracy rules should not block biological control&#39;
u&#39;Animal behaviour: Same-shaped shoals&#39;
u&#39;Genetics: Fast disease factor&#39;
u&#39;Biomimetics: Material monitors mugginess&#39;
u&#39;Cell biology: Lung lipid hurts breathing&#39;
</pre></div>
<p><span class="inline_code">Newsfeed.search()</span> has an optional&nbsp;parameter <span class="inline_code">tags</span>, which is a list of custom tags to parse:</p>
<div class="example">
<pre class="brush: python;gutter: false; light: true; fontsize: 100; first-line: 1; ">&gt;&gt;&gt; for result in Newsfeed().search(NATURE, tags=[&quot;dc:identifier&quot;]):
&gt;&gt;&gt;     print result.dc_identifier</pre></div>
<p>&nbsp;</p>
<hr />
<h2>Google translate</h2>
<p>The <span class="inline_code">Google.translate()</span> method returns the translation of a string in the given language.<br />The <span class="inline_code">Google.identify()</span> method returns a <span class="inline_code">(language code, confidence)</span>-tuple for a given string.&nbsp;</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; s = &quot;C&#39;est un lapin, lapin de bois. Quoi? Un cadeau.&quot;
&gt;&gt;&gt; g = Google()
&gt;&gt;&gt; print g.translate(s, input=&#39;fr&#39;, output=&#39;en&#39;, cached=False)
&gt;&gt;&gt; print g.identify(s)

u&quot;It&#39;s a bunny, rabbit wood. What? A gift.&quot;
(u&#39;fr&#39;, 0.76) </pre></div>
<p>&nbsp;</p>
<hr />
<h2>Twitter trends</h2>
<p>The <span class="inline_code">Twitter.trends()</span> method returns a list of 10 "trending topics":&nbsp;</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; print Twitter().trends(cached=False)

[u&#39;#neverunderstood&#39;, u&#39;Not Top 10&#39;, ...]</pre></div>
<p>&nbsp;</p>
<hr />
<h2><a name="wikipedia"></a>Wikipedia articles</h2>
<p><span class="inline_code">Wikipedia.search()</span> does not return a list of <span class="inline_code">Result</span> objects. Instead, it returns a single <span class="inline_code">MediaWikiArticle</span> for the given (case-sensitive) query. The <span class="inline_code">Wikipedia</span> constructor has an additional <span class="inline_code">language</span> parameter (by default, <span class="inline_code">"en"</span>) that determines the language of the returned articles.</p>
<pre class="brush:python; gutter:false; light:true;">article = MediaWikiArticle(title=&#39;&#39;, source=&#39;&#39;, links=[])</pre><pre class="brush:python; gutter:false; light:true;">article.source              # Article HTML source.
article.string              # Article plaintext unicode string.</pre><pre class="brush:python; gutter:false; light:true;">article.title               # Article title.
article.sections            # Article sections.
article.links               # List of titles of linked articles.
article.external            # List of external links.
article.categories          # List of categories.
article.media               # List of linked media (images, sounds, ...)
article.languages           # Dictionary of (language, article)-items.
article.language            # Article language (i.e. &#39;en&#39;).
article.disambiguation      # True if it is a disambiguation page</pre><pre class="brush:python; gutter:false; light:true;">article.plaintext(**kwargs) # See plaintext() for parameter overview.
article.download(media, **kwargs)
</pre><p><span class="inline_code">MediaWikiArticle.plaintext()</span> is similar to the <span class="inline_code">plaintext()</span> function, but with extra attention for Wikipedia markup. It will remove metadata, infobox, table of contents, annotations, thumbnails and disambiguation links.</p>
<h3>Wikipedia article sections</h3>
<p><span class="inline_code">MediaWikiArticle.sections</span> is a list of <span class="inline_code">MediaWikiSection</span> objects. A section has a title and a number of paragraphs that belong together.</p>
<pre class="brush:python; gutter:false; light:true;">section = MediaWikiSection(article, title=&#39;&#39;, start=0, stop=0, level=1)</pre><pre class="brush:python; gutter:false; light:true;">section.article             # MediaWikiArticle parent.
section.parent              # MediaWikiSection this section is part of.
section.children            # MediaWikiSections belonging to this section.</pre><pre class="brush:python; gutter:false; light:true;">section.title               # Section title.
section.source              # Section HTML source.
section.string              # Section plaintext unicode string.
section.content             # Section string minus title.
section.level               # Section nested depth.
section.tables              # List of MediaWikiTable objects.</pre><p>The following example downloads a Wikipedia article and prints the title of each section, indented according to the section level:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; article = Wikipedia().search(&#39;nodebox&#39;)
&gt;&gt;&gt; for section in article.sections:
&gt;&gt;&gt;    print repr(&#39;  &#39;*(section.level-1) + section.title)

u&#39;NodeBox&#39;
u&#39;  Features&#39;
u&#39;  Supported Primitives&#39;
u&#39;  Output&#39;
u&#39;Libraries&#39;
u&#39;NodeBox 2&#39;
u&#39;NodeBox for OpenGL&#39;
u&#39;Applications&#39;
u&#39;See also&#39;</pre></div>
<h3>Wikipedia article tables</h3>
<p><span class="inline_code">MediaWikiSection.tables</span> is a list of <span class="inline_code">MediaWikiTable</span> objects.&nbsp;A table can have a title, column headers, and rows.&nbsp;</p>
<pre class="brush:python; gutter:false; light:true;">table = MediaWikiTable(section, title=&#39;&#39;, headers=[], rows=[], source=&#39;&#39;)</pre><pre class="brush:python; gutter:false; light:true;">table.section               # MediaWikiSection parent.
table.source                # Table HTML source.
table.title                 # Table title.
table.headers               # List of table column headers.
table.rows                  # List of table rows, each a list of column values.</pre><p>&nbsp;</p>
<hr />
<h2><a name="sort"></a>Web sort</h2>
<p>Interestingly, each list of results returned from <span class="inline_code">SearchEngine.search()</span> has a <span class="inline_code">total</span> property by which we can compare lists. The <span class="inline_code">sort()</span> function sorts the given terms according to search result count.</p>
<pre class="brush:python; gutter:false; light:true;">sort(
    terms = [],             # List of search terms.
  context = &#39;&#39;,             # Term used for sorting.
  service = GOOGLE,         # GOOGLE | BING | YAHOO | FLICKR
  license = None,           # Service license key.
   strict = True,           # Wrap query in quotes?
  reverse = False,
   cached = True)</pre><p>It returns a list of <span class="inline_code">(percentage, term)</span>-tuples for the given list of terms. When a <span class="inline_code">context</span> is defined, sorts according to relevancy to the context: <span class="inline_code">sort(["black", "white"], context="Darth Vader")</span> yields <em>black</em> as the best candidate, because <span class="inline_code">"black Darth Vader"</span> is more common in search results.</p>
<p>Now let's see who is more dangerous:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; results = sort(terms=[
&gt;&gt;&gt;   &#39;arnold schwarzenegger&#39;, 
&gt;&gt;&gt;   &#39;chuck norris&#39;, 
&gt;&gt;&gt;   &#39;dolph lundgren&#39;, 
&gt;&gt;&gt;   &#39;steven seagal&#39;,
&gt;&gt;&gt;   &#39;sylvester stallone&#39;, 
&gt;&gt;&gt;   &#39;mickey mouse&#39;], context=&#39;dangerous&#39;)
&gt;&gt;&gt;
&gt;&gt;&gt; for weight, term in results:
&gt;&gt;&gt;     print &quot;%5.2f&quot; % (weight*100) + &#39;%&#39;, term

43.75% &#39;dangerous chuck norris&#39;
25.00% &#39;dangerous arnold schwarzenegger&#39;
12.50% &#39;dangerous steven seagal&#39;
12.50% &#39;dangerous mickey mouse&#39;
 6.25% &#39;dangerous sylvester stallone&#39;
 0.00% &#39;dangerous dolph lundgren&#39;
</pre></div>
<p>&nbsp;</p>
<hr />
<h2><a name="plaintext"></a>HTML to plaintext</h2>
<p>Typically, <span class="inline_code">URL.download()</span> is used to retrieve HTML documents. HTML is a markup language that uses <em>tags</em> to define the text formatting. For example: <span class="inline_code">&lt;b&gt;hello&lt;/b&gt;</span> displays <strong>hello</strong> in bold. Usually, we just want the text without the formatting so we can analyze (e.g. <a href="pattern-en.html#parser">parse</a>) it.</p>
<p>The <span class="inline_code">plaintext()</span> function removes HTML formatting from a string.</p>
<pre class="brush:python; gutter:false; light:true;">plaintext(html, keep=[], replace=blocks, linebreaks=2, indentation=False)</pre><p>It will perform the following steps to clean up the given string:</p>
<ul>
<li><strong>Strip javascript:</strong> remove all <span class="inline_code">&lt;script&gt;</span> elements.</li>
<li><strong>Strip CSS: </strong>remove all <span class="inline_code">&lt;style&gt;</span> elements.</li>
<li><strong>Strip comments:</strong> remove all <span class="inline_code">&lt;!-- --&gt;</span> elements.</li>
<li><strong>Strip forms: </strong>remove all <span class="inline_code">&lt;form&gt;</span> elements.</li>
<li><strong>Strip tags: </strong>remove all HTML tags.</li>
<li><strong>Decode entities:</strong> replace <span class="inline_code">&amp;lt;</span> with <span class="inline_code">&lt;</span> (for example).</li>
<li><strong>Collapse spaces:</strong> consecutive spaces are replaced with a single space.</li>
<li><strong>Collapse linebreaks:</strong> consecutive linebreaks are replace with a single linebreak.</li>
<li><strong>Collapse tabs:</strong> consecutive tabs are replaced by a single space, optionally indentation (tabs at the start of a line) can be preserved.</li>
</ul>
<p><span class="smallcaps">plaintext tweaks</span></p>
<p>The <span class="inline_code">keep</span> parameter is a list of tags to keep. By default, element attributes are stripped, e.g. <span class="inline_code">&lt;table border="0"&gt;</span>&nbsp; becomes&nbsp; <span class="inline_code">&lt;table&gt;</span>. To preserve specific attributes, instead of a list a dictionary can be passed: <span class="inline_code">{"a": ["href"]}</span>.</p>
<p>The <span class="inline_code">replace</span> parameter defines how HTML elements are replaced with other characters to improve plain text layout. It is a dictionary of <span class="inline_code">tag</span> → <span class="inline_code">(before, after)</span> items. The default <span class="inline_code">blocks</span> dictionary replaces block elements (<span class="inline_code">&lt;h1&gt;</span>, <span class="inline_code">&nbsp;</span><span class="inline_code">&lt;h2&gt;</span>, <span class="inline_code">&nbsp;</span><span class="inline_code">&lt;p&gt;</span>, <span class="inline_code">&nbsp;</span><span class="inline_code">&lt;div&gt;</span>, <span class="inline_code">&nbsp;</span><span class="inline_code">&lt;table&gt;</span>, ...) with two linebreaks after, <span class="inline_code">&lt;th&gt;</span> and <span class="inline_code">&lt;tr&gt;</span> with one linebreak after, <span class="inline_code">&lt;td&gt;</span> with one tab after, <span class="inline_code">&lt;li&gt;</span> with an asterisk (<span class="inline_code">*</span>) before and a linebreak after.</p>
<p>The <span class="inline_code">linebreaks</span> parameter defines the maximum amount of consecutive linebreaks to keep.</p>
<p>The <span class="inline_code">indentation</span> parameter defines whether or not to keep tab indentation.</p>
<p>For example, the following script downloads a HTML document and keeps only a minimal amount of formatting (headings, bold, links).</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; s = URL(&#39;http://www.clips.ua.ac.be&#39;).download()
&gt;&gt;&gt; s = plaintext(s, keep={&#39;h1&#39;:[], &#39;h2&#39;:[], &#39;strong&#39;:[], &#39;a&#39;:[&#39;href&#39;]})
&gt;&gt;&gt; print s
</pre></div>
<p style="margin-top: 1.3em;"><span class="smallcaps">plaintext = strip + decode + collapse</span></p>
<p>The different steps in <span class="inline_code">plaintext()</span> are also available as separate functions:</p>
<pre class="brush:python; gutter:false; light:true;">decode_utf8(string)         # Byte string to Unicode string.</pre><pre class="brush:python; gutter:false; light:true;">encode_utf8(string)         # Unicode string to byte string.
</pre><pre class="brush:python; gutter:false; light:true;">strip_tags(html, keep=[], replace=blocks) # Non-trivial, using SGML parser.
</pre><pre class="brush:python; gutter:false; light:true;">strip_between(a, b, string) # Remove anything between (and including) a and b.
</pre><pre class="brush:python; gutter:false; light:true;">strip_javascript(html)      # Strips between &#39;&lt;script*&gt;&#39; and &#39;&lt;/script&#39;.</pre><pre class="brush:python; gutter:false; light:true;">strip_inline_css(html)      # Strips between &#39;&lt;style*&gt;&#39; and &#39;&lt;/style&gt;&#39;.</pre><pre class="brush:python; gutter:false; light:true;">strip_comments(html)        # Strips between &#39;&lt;!--&#39; and &#39;--&gt;&#39;.</pre><pre class="brush:python; gutter:false; light:true;">strip_forms(html)           # Strips between &#39;&lt;form*&gt;&#39; and &#39;&lt;/form&gt;&#39;.</pre><pre class="brush:python; gutter:false; light:true;">decode_entities(string)     # &#39;&amp;lt;&#39; =&gt; &#39;&lt;&#39;</pre><pre class="brush:python; gutter:false; light:true;">encode_entities(string)     # &#39;&lt;&#39; =&gt; &#39;&amp;lt;&#39;</pre><pre class="brush:python; gutter:false; light:true;">collapse_spaces(string, indentation=False, replace=&#39; &#39;)</pre><pre class="brush:python; gutter:false; light:true;">collapse_tabs(string, indentation=False, replace=&#39; &#39;)</pre><pre class="brush:python; gutter:false; light:true;">collapse_linebreaks(string, threshold=1)</pre><p>&nbsp;</p>
<hr />
<h2 class="example"><a name="DOM"></a>HTML DOM parser</h2>
<p>The <em>Document Object Model</em> (DOM) is a cross-platform and language-independent convention for representing and interacting with objects in HTML, XHTML and XML documents. The web module includes a HTML DOM parser (Leonard Richardson's <a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank">BeautifulSoup</a>), that allows you to traverse a HTML document as a tree of linked Python objects. This is useful to extract specific portions from a HTML string retrieved with <span class="inline_code">URL.download()</span>.</p>
<h3>Node</h3>
<p>The DOM consists of a <span class="inline_code">DOM</span>&nbsp;object that contains <span class="inline_code">Text</span>, <span class="inline_code">Comment</span> and <span class="inline_code">Element</span> objects.<br />All of these are subclasses of <span class="inline_code">Node</span>.</p>
<pre class="brush:python; gutter:false; light:true;">node = Node(html, type=NODE)</pre><pre class="brush:python; gutter:false; light:true;">node.type                   # NODE | TEXT | COMMENT | ELEMENT | DOCUMENT
node.source                 # HTML source.
node.parent                 # Parent node.
node.children               # List of child nodes.
node.next                   # Next child in node.parent (or None).
node.previous               # Previous child in node.parent (or None).</pre><pre class="brush:python; gutter:false; light:true;">node.traverse(visit=lambda node: None)</pre><h3>Element</h3>
<p><span class="inline_code">Text</span>, <span class="inline_code">Comment</span> and <span class="inline_code">Element</span> are subclasses of <span class="inline_code">Node</span>. For example: <span class="inline_code">'the &lt;b&gt;cat&lt;/b&gt;'</span> is parsed to <span class="inline_code">Text('the')</span> + <span class="inline_code">Element('cat', tag='b')</span>. The <span class="inline_code">Element</span> object has a number of additional properties:</p>
<pre class="brush:python; gutter:false; light:true;">element = Element(html)</pre><pre class="brush:python; gutter:false; light:true;">element.tag                 # Tag name.
element.attributes          # Dictionary of attributes, e.g. {&#39;class&#39;:&#39;menu&#39;}.
element.id                  # Value for id attribute (or None).</pre><pre class="brush:python; gutter:false; light:true;">element.source              # HTML source.
element.content             # HTML source minus open and close tag.</pre><pre class="brush:python; gutter:false; light:true;">element.by_id(str)          # First nested Element with given id.
element.by_tag(str)         # List of nested Elements with given tag name.
element.by_class(str)       # List of nested Elements with given class.
element.by_attribute(**kwargs)
</pre><ul>
<li><span class="inline_code">Element.by_tag()</span> values can include a class (e.g. <span class="inline_code">"div.header"</span>) or an id (e.g. <span class="inline_code">"div#content"</span>). <br />A wildcard can be used to match any tag. (e.g. <span class="inline_code">"*.even"</span>).<br />The element is searched recursively (children in children, etc.)</li>
<li><span class="inline_code">Element.by_attribute()</span> takes one or more keyword arguments (e.g. <span class="inline_code">name="keywords"</span>).</li>
</ul>
<h3>DOM</h3>
<p>The top-level element in the Document Object Model.</p>
<pre class="brush:python; gutter:false; light:true;">dom = DOM(html)</pre><pre class="brush:python; gutter:false; light:true;">dom.declaration             # &lt;!doctype&gt; TEXT Node.
dom.head                    # &lt;head&gt; Element.
dom.body                    # &lt;body&gt; Element.</pre><p>For example, the following script retrieves the last three entries from <a href="http://www.reddit.com/" target="_blank">reddit</a>. The web module does not include a reddit search engine, but we can parse entries directly from the HTML source. This is called <em>screen scraping</em>, and many websites will strongly dislike it (server pressure).</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; url = URL(&#39;http://www.reddit.com/top/&#39;)
&gt;&gt;&gt; dom = DOM(url.download(cached=True))
&gt;&gt;&gt; for e in dom.by_tag(&#39;div.entry&#39;)[:3]: # Top 3 reddit entries.
&gt;&gt;&gt;     for a in e.by_tag(&#39;a.title&#39;)[:1]: # First &lt;a class=&quot;title&quot;&gt;.
&gt;&gt;&gt;         print repr(plaintext(a.content))

u&#39;Invisible Kitty&#39;
u&#39;Naturally, he said yes.&#39;
u&quot;I&#39;d just like to remind everyone that /r/minecraft exists and not everyone wants&quot; 
 &quot;to have 10 Minecraft posts a day on their front page.&quot;</pre></div>
<p><span class="smallcaps"><br />Absolute url's</span></p>
<p>Links parsed from the <span class="inline_code">DOM</span> may be relative (e.g., starting with <span class="inline_code">"../"</span> instead of <span class="inline_code">"http://"</span>. <br />To get the absolute URL, you can use the <span class="inline_code">abs()</span> function in combination with <span class="inline_code">URL.redirect</span>:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.web import abs
&gt;&gt;&gt; url = URL(&#39;http://nodebox.net&#39;)
&gt;&gt;&gt; dom = DOM(url.download())
&gt;&gt;&gt; for link in dom.by_tag(&#39;a&#39;):
&gt;&gt;&gt;     print abs(link.attributes.get(&#39;href&#39;,&#39;&#39;), base=url.redirect or url.string) </pre></div>
<p>&nbsp;</p>
<hr />
<h2><a name="pdf"></a>PDF Parser</h2>
<p style="margin-top: 0.2em; margin-right: 0px; margin-bottom: 0.5em; margin-left: 0px;"><em>Portable Document Format</em>&nbsp;(PDF) is a popular open standard for document exchange. The text, fonts, images and layout are contained in a single document that displays the same across systems. However, extracting the source text from a PDF is a non-trivial matter.</p>
<p style="margin-top: 0.2em; margin-right: 0px; margin-bottom: 0.5em; margin-left: 0px;">The <span class="inline_code">PDF</span> object (based on <a href="http://www.unixuser.org/~euske/python/pdfminer/" target="_self">PDFMiner</a>) parses the source text from a PDF file. This is useful for mining the text, not so much for displaying it (i.e., formatting is lost and some passages may have become garbled).</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.web import URL, PDF
&gt;&gt;&gt; url = URL(&#39;http://www.clips.ua.ac.be/sites/default/files/ctrs-002_0.pdf&#39;)
&gt;&gt;&gt; pdf = PDF(url.download())
&gt;&gt;&gt; print pdf.string

CLiPS Technical Report series 002 September 7, 2010
Tom De Smedt, Vincent Van Asch, Walter Daelemans 
Computational Linguistics &amp; Psycholinguistics Research Center
... </pre></div>
<p style="margin-top: 0.2em; margin-right: 0px; margin-bottom: 0.5em; margin-left: 0px;">URL's linking to a PDF document can be identified with&nbsp;<span class="inline_code">URL.mimetype in MIMETYPE_PDF</span>.</p>
<p>&nbsp;</p>
<hr />
<h2><a name="spider"></a>Spider</h2>
<p>A <em>web crawler</em> or <em>web spider</em> is used to browse the web in an automatter manner. The <span class="inline_code">Spider</span> class is initialized with a list of URLs. These are then visited by the spider. If they lead to a web page (i.e. HTML), the content is parsed for new links. These are added to a list of links scheduled for a visit.</p>
<p>The given <span class="inline_code">domains</span> is a list of allowed domain names. An empty list means the spider can visit the entire web. The given <span class="inline_code">delay</span> defines the amount of seconds to wait before revisiting the same (sub)domain. This is a politeness policy: continually hammering a server with a robot disrupts requests from the website's regular visitors (this is called a <em>denial-of-service attack</em>).</p>
<pre class="brush:python; gutter:false; light:true;">spider = Spider(links=[], domains=[], delay=20.0, parser=HTMLLinkParser().parse)</pre><pre class="brush:python; gutter:false; light:true;">spider.domains              # Domains allowed to visit (e.g., [&#39;clips.ua.ac.be&#39;]).
spider.delay                # Delay between visits to the same (sub)domain.
spider.history              # Dictionary of (domain, time last visited).
spider.visited              # Dictionary of URLs visited.
spider.parse                # Function, returns list of Links from a HTML string.
spider.sort                 # FIFO | LIFO (how new links are queued).
spider.done                 # True when all links have been visited.</pre><pre class="brush:python; gutter:false; light:true;">spider.crawl(method=DEPTH)  # DEPTH | BREADTH | None.</pre><pre class="brush:python; gutter:false; light:true;">spider.priority(link, method=DEPTH)
spider.follow(link)
spider.visit(link, source=None)
spider.fail(link)
</pre><pre class="brush:python; gutter:false; light:true;">spider.normalize(url)
</pre><p><span class="smallcaps">Crawling process</span></p>
<ul>
<li><span class="inline_code">Spider.crawl()</span>&nbsp;is meant to be called continuously in a loop.&nbsp;&nbsp;It&nbsp;selects a link to visit, and parses its content for new links. The default parser is a robust <span class="inline_code">HTMLLinkParser</span> based on Python's <span class="inline_code">sgmllib.SGMLParser</span>. The method parameter specifies whether the spider prefers to visit internal links (<span class="inline_code">DEPTH</span>) or external links to other domains (<span class="inline_code">BREADTH</span>).<br />&nbsp;<br />If the link is on a domain recently visited (elapsed time &lt; <span class="inline_code">Spider.delay</span>) it is temporarily skipped. If this is undesired, set an optional <span class="inline_code">throttle</span> parameter of <span class="inline_code">Spider.crawl()</span> to the same value as <span class="inline_code">Spider.delay</span>.</li>
</ul>
<ul>
<li><span class="inline_code">Spider.priority()</span> is called from <span class="inline_code">Spider.crawl()</span> to determine the priority of a new <span class="inline_code">Link</span>, as a number between <span class="inline_code">0.0-1.0</span>. Links with higher priority are visited first.&nbsp;<span class="inline_code" style="font-family: Courier, monospace; font-size: 12px;">Spider.priority()</span>&nbsp;can be overridden in a subclass, for example to demote URLs with a query string (which could just be another sort order).&nbsp;Each URL is passed through&nbsp;<span class="inline_code" style="font-family: Courier, monospace; font-size: 12px;">Spider.normalize()</span>which can also be overridden (for example, to strip the query string entirely).</li>
</ul>
<ul>
<li><span class="inline_code">Spider.follow()</span> is called from <span class="inline_code">Spider.crawl()</span> to determine if it should visit the given <span class="inline_code">Link</span>. By default it yields <span class="inline_code">True</span>, but it can be overridden to disallow selected links.</li>
</ul>
<ul>
<li><span class="inline_code">Spider.visit()</span> is called from <span class="inline_code">Spider.crawl()</span> once a <span class="inline_code">Link</span> is visited. The&nbsp;<span class="inline_code">source</span>&nbsp;will be an HTML string with the content.&nbsp;By default this method does nothing, but it can be overridden.</li>
</ul>
<ul>
<li><span class="inline_code">Spider.fail()</span> is called from <span class="inline_code">Spider.crawl()</span> for links whose MIME-type could not be determined or which raised a <span class="inline_code">URLError</span> on download.</li>
</ul>
<p>The spider uses <span class="inline_code">Link</span> objects internally, which hold additional information besides the URL string:</p>
<pre class="brush:python; gutter:false; light:true;">link = Link(url, description=&#39;&#39;, relation=&#39;&#39;)</pre><pre class="brush:python; gutter:false; light:true;">link.url                    # Parsed from &lt;a href=&#39;&#39;&gt; attribute.
link.description            # Parsed from &lt;a title=&#39;&#39;&gt; attribute.
link.relation               # Parsed from &lt;a rel=&#39;&#39;&gt; attribute.
link.referrer               # Parent web page URL.</pre><p>For example, here is a subclass of <span class="inline_code">Spider</span> that simply prints each link it visits. Since it uses <span class="inline_code">DEPTH</span> for crawling, it will prefer website internal links.</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; class Spiderling(Spider): 
&gt;&gt;&gt;     def visit(self, link, source=None):
&gt;&gt;&gt;         print &#39;visited:&#39;, repr(link.url), &#39;from:&#39;, link.referrer
&gt;&gt;&gt;     def fail(self, link):
&gt;&gt;&gt;         print &#39;failed:&#39;, repr(link.url)
&gt;&gt;&gt;
&gt;&gt;&gt; s = Spiderling(links=[&#39;http://www.clips.ua.ac.be/&#39;], delay=5, queue=True)
&gt;&gt;&gt; while not s.done:
&gt;&gt;&gt;     s.crawl(method=DEPTH, cached=False, throttle=5)

visited: u&#39;http://www.clips.ua.ac.be/&#39;
visited: u&#39;http://www.clips.ua.ac.be/#navigation&#39;
visited: u&#39;http://www.clips.ua.ac.be/colloquia&#39;
visited: u&#39;http://www.clips.ua.ac.be/computational-linguistics&#39;
visited: u&#39;http://www.clips.ua.ac.be/contact&#39;
</pre></div>
<p><span class="small"><span style="text-decoration: underline;">Note</span>: <span class="inline_code">Spider.crawl()</span> takes the same parameters as <span class="inline_code">URL.download()</span>, e.g. </span><span class="small"><span class="inline_code">cached=False</span> or <span class="inline_code">throttle=10</span>.</span></p>
<p>&nbsp;</p>
<hr />
<h2><a name="mail"></a>E-mail</h2>
<p>The <em>Internet Message Access Protocol </em>(IMAP) is a protocol for retrieving e-mail messages from a mail server or webmail service.<em> </em>The <span class="inline_code">Mail</span> object is a wrapper for the Python <span class="inline_code">imaplib</span>. It can be used to search + read e-mail messages from your webmail.</p>
<p>Currently, it supports one service: <span class="inline_code">GMAIL</span>. However, it may work with other services by passing the server address to the <span class="inline_code">service</span> parameter (e.g. <span class="inline_code">service="imap.gmail.com"</span>). Note that you need to <a href="http://mail.google.com/support/bin/answer.py?answer=77695" target="_blank">enable IMAP</a> from your Gmail account if you want to access it.</p>
<p>With <span class="inline_code">secure=False</span> (no SSL) the default <span class="inline_code">port</span> is 143.</p>
<pre class="brush:python; gutter:false; light:true;">mail = Mail(username, password, service=GMAIL, port=993, secure=True)</pre><pre class="brush:python; gutter:false; light:true;">mail.folders               # Dictionary of name =&gt; MailFolder.
mail.[folder]              # For example: Mail.inbox.read(i)
mail.[folder].count        # Number of messages in folder.
</pre><pre class="brush:python; gutter:false; light:true;">mail.[folder].search(query, field=FROM) # FROM | SUBJECT | DATE
mail.[folder].read(index, attachments=False, cached=True)</pre><p>E-mail messages are organized in folders. The Mail.folders is a <span class="inline_code">name</span> → <span class="inline_code">MailFolder</span> dictionary. Folders can also be accessed directly by name, as an attribute. Common names include: <span class="inline_code">inbox</span>, <span class="inline_code">spam</span>, <span class="inline_code">trash</span>.<span class="inline_code">&nbsp;</span></p>
<p><span class="inline_code">MailFolder.search()</span> returns a list of e-mail indices, latest-first.<span class="inline_code"><br />MailFolder.read()</span> retrieves the e-mail with given index as a <span class="inline_code">Message</span>:</p>
<pre class="brush:python; gutter:false; light:true;">message = Mail.[folder].read(i)</pre><pre class="brush:python; gutter:false; light:true;">message.author              # Unicode string, sender name + e-mail address.
message.email_address       # Unicode string, sender e-mail address.
message.date                # Unicode string, date received.
message.subject             # Unicode string, message subject.
message.body                # Unicode string, message body.
message.attachments         # List of (MIME-type, str)-tuples.
</pre><p>For example:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.web import Mail, GMAIL, SUBJECT
&gt;&gt;&gt; gmail = Mail(username=&#39;me&#39;, password=&#39;secret&#39;, service=GMAIL)
&gt;&gt;&gt; print gmail.folders.keys()

[&#39;drafts&#39;, &#39;spam&#39;, &#39;personal&#39;, &#39;work&#39;, &#39;inbox&#39;, &#39;mail&#39;, &#39;starred&#39;, &#39;trash&#39;]</pre></div>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; i = gmail.spam.search(&#39;wish&#39;, field=SUBJECT)[0] # What riches await...
&gt;&gt;&gt; m = gmail.spam.read(i)
&gt;&gt;&gt; print &#39;   From:&#39;, m.author
&gt;&gt;&gt; print &#39;Subject:&#39;, m.subject
&gt;&gt;&gt; print &#39;Message:&#39;
&gt;&gt;&gt; print m.body

   From: Vegas VIP Clib &lt;amllhbmjb@acciongeoda.org&gt;
Subject: Your wish has been granted
Message:
No one has claimed our jackpot! This is your chance to try! 
http://www.top-hot-casino.ru</pre></div>
<p>&nbsp;</p>
<hr />
<h2><a name="locale"></a>Locale</h2>
<p>The helper module&nbsp;<span class="inline_code">pattern.web.locale</span>&nbsp;contains functions for region and language codes, based on the ISO-639 language code (e.g., <span class="inline_code">en</span>), the ISO-3166 region code (e.g., <span class="inline_code">US</span>) and the IETF BCP 47 language-region specification (<span class="inline_code">en-US</span>):</p>
<pre class="brush:python; gutter:false; light:true;">encode_language(name)       # &#39;English&#39; =&gt; &#39;en&#39;</pre><pre class="brush:python; gutter:false; light:true;">decode_language(code)       # &#39;en&#39; =&gt; &#39;English&#39;</pre><pre class="brush:python; gutter:false; light:true;">encode_region(name)         # &#39;United States&#39; =&gt; &#39;US&#39;</pre><pre class="brush:python; gutter:false; light:true;">decode_region(code)         # &#39;US&#39; =&gt; &#39;United States&#39;</pre><pre class="brush:python; gutter:false; light:true;">languages(region)           # &#39;US&#39; =&gt; [&#39;en&#39;]</pre><pre class="brush:python; gutter:false; light:true;">regions(language)           # &#39;en&#39; =&gt; [&#39;GB&#39;, &#39;NZ&#39;, &#39;TT&#39;, ...]</pre><pre class="brush:python; gutter:false; light:true;">regionalize(language)       # &#39;en&#39; =&gt; [&#39;en-US&#39;, &#39;en-GB&#39;, ...]</pre><pre class="brush:python; gutter:false; light:true;">market(language)            # &#39;en&#39; =&gt; &#39;en-US&#39;</pre><p>The <span class="inline_code">geocode()</span> function recognizes a number of world capital cities and returns a tuple&nbsp;(<span class="inline_code">latitude</span>, <span class="inline_code">longitude</span>, <span class="inline_code">ISO-639</span>, <span class="inline_code">region</span>).</p>
<pre class="brush:python; gutter:false; light:true;">geocode(location)           # &#39;Brussels&#39; =&gt; (50.83, 4.33, u&#39;nl&#39;, u&#39;Belgium&#39;)</pre><p>This is useful in combination with the&nbsp;<span class="inline_code">geo</span> parameter for <span class="inline_code">Twitter.search()</span> to obtain regional tweets:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.web import Twitter
&gt;&gt;&gt; from pattern.web.locale import geocode 
&gt;&gt;&gt; twitter = Twitter(language=&#39;en&#39;)
&gt;&gt;&gt; for tweet in twitter.search(&#39;restaurant&#39;, geo=geocode(&#39;Brussels&#39;)[:2]):
&gt;&gt;&gt;      print tweet.description

u&#39;Did you know: every McDonalds restaurant has free internet in Belgium...&#39;</pre></div>
<p>&nbsp;</p>
<hr />
<h2><a name="cache"></a>Cache</h2>
<p>By, default, <span class="inline_code">URL.download()</span> and <span class="inline_code">SearchEngine.search()</span> will cache results locally. This way, there is no need to connect to the internet once a query has been cached. Over time the cache can grow quite large, filled with whatever was downloaded – from Wikipedia pages to zip archives.</p>
<p>Emptying it is easy (and permanent):</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.web import cache
&gt;&gt;&gt; cache.clear()
</pre></div>
<p>&nbsp;</p>
<hr />
<h2>See also</h2>
<ul>
<li><a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank">BeautifulSoup</a> (BSD): r<span>obust HTML parser for Python.</span></li>
<li><span><a href="http://scrapy.org/" target="_blank">Scrapy</a> (BSD): s</span><span>creen scraping and web crawling with Python.</span></li>
</ul>
</div>
</div></div>
        </div>
    </div>
    </div>
    </div>
    </div>
    </div>
    <script>
        SyntaxHighlighter.all();
    </script>
</body>
</html>